#Little pause for some calculations about our classification scheme...
foo <- alldata[alldata$Final.Full.WordOrder != alldata$WordOrder & alldata$WordOrder.Classified != "Unclassified",]$Final.Full.WordOrder
goo <- as.data.frame(foo)
goo$len <- unlist(lapply(foo, nchar))
goo <- goo[goo$len > 3,]
goo[order(goo$foo),]
#(And do some manual checking about items that might have been misclassified)
#OK: For items longer than length 3 that DID get classified, here are places we might have mistakes:
# OVOSV - maybe not SOV
# SOVSVO 2 - maybe not SVO
####
# Now do it by scores-per-participant. (Need this to make the bootstrapped confidence intervals...)
#Drop Unclassified items!!
alldata <- alldata[!(alldata$WordOrder.Classified == "Unclassified"),]
alldata <- alldata[!(alldata$SpatialCue == "?"),]
#Optionally, drop non SuperGoodResponses!
#alldata <- alldata[alldata$SuperGoodResponse == "Yes",]
#Make scores for each participant
alldata$ChoseLateral <- 0
alldata[alldata$WordOrder.Classified == "VerbLateral",]$ChoseLateral <- 1
ParticipantScores <- aggregate(alldata$ChoseLateral, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(ParticipantScores) <- c("Subject", "Object.Type", "GestureCondition", "ChoseLateral")
#Table for scores too
with(ParticipantScores, tapply(ChoseLateral, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
#Time for bootstrapped confidence intervals around the means of the 4 conditions!
PersonFree.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Person" & ParticipantScores$GestureCondition=="Free",]$ChoseLateral, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
PersonHand.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Person" & ParticipantScores$GestureCondition=="Case",]$ChoseLateral, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
ObjectFree.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Object" & ParticipantScores$GestureCondition=="Free",]$ChoseLateral, 1000, mean)
quantile(ObjectFree.boot.mean$thetastar, c(0.025, 0.975))
ObjectHand.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Object" & ParticipantScores$GestureCondition=="Case",]$ChoseLateral, 1000, mean)
quantile(ObjectHand.boot.mean$thetastar, c(0.025, 0.975))
#And scores on Spatial stuff!
alldata$Casemarked <- 0
alldata[alldata$SpatialCue == "Spatial.Present",]$Casemarked <- 1
SpatialScores <- aggregate(alldata$Casemarked, by=list(alldata$Subject, alldata$WordOrder.Classified, alldata$GestureCondition), mean.na.rm)
names(SpatialScores) <- c("Subject", "WordOrder.Classified", "GestureCondition", "Casemarked")
#Table for scores too
with(SpatialScores, tapply(Casemarked, list(WordOrder.Classified, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
#Time for bootstrapped confidence intervals around the means of the 4 conditions!
PersonFree.boot.mean = bootstrap(SpatialScores[SpatialScores$WordOrder.Classified=="VerbLateral" & SpatialScores$GestureCondition=="Free",]$Casemarked, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
PersonHand.boot.mean = bootstrap(SpatialScores[SpatialScores$WordOrder.Classified=="VerbLateral" & SpatialScores$GestureCondition=="Case",]$Casemarked, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
ObjectFree.boot.mean = bootstrap(SpatialScores[SpatialScores$WordOrder.Classified=="VerbMedial" & SpatialScores$GestureCondition=="Free",]$Casemarked, 1000, mean)
quantile(ObjectFree.boot.mean$thetastar, c(0.025, 0.975))
ObjectHand.boot.mean = bootstrap(SpatialScores[SpatialScores$WordOrder.Classified=="VerbMedial" & SpatialScores$GestureCondition=="Case",]$Casemarked, 1000, mean)
quantile(ObjectHand.boot.mean$thetastar, c(0.025, 0.975))
#And did they casemark differently depending on animacy?
AnimacySpatialScores <- aggregate(alldata$Casemarked, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(AnimacySpatialScores) <- c("Subject", "Object.Type", "GestureCondition", "Casemarked")
with(AnimacySpatialScores, tapply(Casemarked, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
#########################################
##STATISTICAL TESTS!!
#########################################
alldata$WordOrder.Classified <- as.factor(alldata$WordOrder.Classified)
alldata$Object.Type <- as.factor(alldata$Object.Type)
alldata$SpatialCue <- as.factor(alldata$SpatialCue)
alldata$GestureCondition <- as.factor(alldata$GestureCondition)
freedata <- alldata[alldata$GestureCondition == "Free",]
handdata <- alldata[alldata$GestureCondition == "Case",]
freedata$WordOrder.Classified <- as.factor(freedata$WordOrder.Classified)
handdata$WordOrder.Classified <- as.factor(handdata$WordOrder.Classified)
freedata$Object.Type <- as.factor(freedata$Object.Type)
handdata$Object.Type <- as.factor(handdata$Object.Type)
#Within Experiments- were people more likely to use SVO with Animate?
free_model <- lmer(WordOrder.Classified ~ Object.Type  + (1+Object.Type|Subject), data=freedata, family="binomial")
summary(free_model)
hand_model <- lmer(WordOrder.Classified ~ Object.Type  + (1+Object.Type|Subject), data=handdata, family="binomial")
summary(hand_model)
#Between Experiments! - was there an interaction between Experiment and the above?
word_order_model <- lmer(WordOrder.Classified ~ Object.Type*GestureCondition  + (1+Object.Type|Subject), data=alldata, family="binomial")
summary(word_order_model)
#Between cues  - was there an interaction between ACTUALLY USING SPACE and useing SVO for animates?
spatial_model <- lmer(SpatialCue ~ WordOrder.Classified  + (1+Object.Type|Subject), data=alldata, family="binomial")
summary(spatial_model)
word_order_spatial_model <- lmer(WordOrder.Classified ~ Object.Type*SpatialCue  + (1+Object.Type|Subject), data=alldata, family="binomial")
summary(word_order_spatial_model)
#Did being animate make you more likely to casemark? No.  It looks like people took it as a general strategy.
cue_model <- lmer(SpatialCue ~ Object.Type*GestureCondition  + (1+Object.Type|Subject), data=alldata, family="binomial")
summary(cue_model)
######
# A new thing. Need more detail about the casemarking in the casemarked trials! To facilitate, print out
# a new file, SpatialCasemarking_Casemarked_Trials.csv, with just the (legal subject) casemarking
# trials.
orig_all_data <- read.csv(paste0(directory, "/SpatialCasemarking_AllGestureData.csv"), header = TRUE)
alldata_short <- alldata[,c("Subject", "Trial.Number","GestureCondition","SpatialCue", "Final.Full.WordOrder")]
foo <- merge(orig_all_data, alldata_short, by=c("Subject", "Trial.Number","GestureCondition"))
foo$SpatialCue.FinalDecision <- foo$SpatialCue
foo <- foo[foo$SpatialCue.FinalDecision == "Spatial.Present",]
write.csv(foo, file = paste0(directory, "/SpatialCasemarking_OnlyCaseTrials.csv"))
#####
# Another new thing.  We want to check if Embodiment (in the second task)
# made a difference for SOV use (ie maybe we accidentally did an embodiment
# manipulation...).  For this, read in the new Embodiment coding that Miguel
# did ~ 10/22/14
embodiment_data <- read.csv(paste0(directory, "/EmbodimentHallRecode.csv"), header = TRUE)
#drop some duplicate columns we dont' need...
embodiment_data <- embodiment_data[,c("Clipped.Movie.File","Trial.Number", "Agent.Embod","Verb.Embod","Patient.Embod")]
alldata <- merge(alldata, embodiment_data, by=c("Clipped.Movie.File","Trial.Number"),all.X=TRUE, all.y=FALSE)
#Did Embodiment change across the 2 conditions? Check Agent and Verb
table(alldata$Agent.Embod, alldata$Object.Type, alldata$GestureCondition)
table(alldata$Verb.Embod, alldata$Object.Type, alldata$GestureCondition)
table(alldata$Patient.Embod, alldata$Object.Type, alldata$GestureCondition)
#Fascinating! Agent and Patient embodiment is actually about the same,
# but verb is very different:
#For verb, Embodiment is 0 on many more trials in Case.  But happily, there it's
#about half and half, so we should be able to measure effects! Let's quantify that...
#OK, I thought about it: Embodicment hyp is centrally about avoiding role
#conflict between V and O.  This means a) gotta recode WordOrder, and b)
# the metric we care about is intersect(V,O)
#Pilot: drop ?s!
alldata <- alldata[alldata$Agent.Embod != "?",]
alldata <- alldata[alldata$Verb.Embod != "?",]
alldata <- alldata[alldata$Patient.Embod != "?",]
alldata$Agent.Embod <- as.numeric(as.character(alldata$Agent.Embod))
alldata$Verb.Embod <- as.numeric(as.character(alldata$Verb.Embod))
alldata$Patient.Embod <- as.numeric(as.character(alldata$Patient.Embod))
alldata$PV.Embod <- (alldata$Verb.Embod == 1) & (alldata$Patient.Embod == 1)
EmbodAgentScores <- aggregate(alldata$Agent.Embod, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(EmbodAgentScores) <- c("Subject", "Object.Type", "GestureCondition", "Agent.Embod")
EmbodVerbScores <- aggregate(alldata$Verb.Embod, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(EmbodVerbScores) <- c("Subject", "Object.Type", "GestureCondition", "Verb.Embod")
EmbodPatientScores <- aggregate(alldata$Patient.Embod, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(EmbodPatientScores) <- c("Subject", "Object.Type", "GestureCondition", "Patient.Embod")
EmbodPVScores <- aggregate(alldata$PV.Embod, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(EmbodPVScores) <- c("Subject", "Object.Type", "GestureCondition", "PV.Embod")
#Table for scores too
with(EmbodAgentScores, tapply(Agent.Embod, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
with(EmbodVerbScores, tapply(Verb.Embod, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
with(EmbodPatientScores, tapply(Patient.Embod, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
with(EmbodPVScores, tapply(PV.Embod, list(Object.Type, GestureCondition), mean, na.rm=TRUE), drop=TRUE)
#OK! So it looks like our manipulation did decrease PV from Free (65%) to Case (30%)
#For now, just use the orginal WordOrder classifications, though that's slightly wrong
#Add New WordOrder classifications! The distinction is whether s is the last
#entity before the v.
alldata$WordOrder.Embod.Classified <- "Unclassified"
alldata[alldata$WordOrder == "SOV",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "OSV",]$WordOrder.Embod.Classified <- "Adjacent"
alldata[alldata$WordOrder == "VSO",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "VOS",]$WordOrder.Embod.Classified <- "NonAdjacent"
#Parenthesis cases
alldata[alldata$WordOrder == "V(OS)",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "V(SO)",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "(SO)V",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "(OS)V",]$WordOrder.Embod.Classified <- "NonAdjacent"
alldata[alldata$WordOrder == "SVO",]$WordOrder.Embod.Classified <- "Adjacent"
alldata[alldata$WordOrder == "OVS",]$WordOrder.Embod.Classified <- "NonAdjacent"
#OK, now let's look within the Casemarking task.  Did the people who
#Embodied their (verb/patient) gestures use more SVO for AnimatePatients?
casedata <- alldata[alldata$GestureCondition =='Case',]
freedata <- alldata[alldata$GestureCondition =='Free',]
table(casedata$WordOrder.Embod.Classified, casedata$Object.Type, casedata$PV.Embod)
table(freedata$WordOrder.Embod.Classified, freedata$Object.Type, freedata$PV.Embod)
#OK, so looks like in case, most of the Person ones are still Lateral even if
#conflict is present.  Let's check if they are the weird Matt hall ones!
casedata[casedata$PV.Embod == FALSE,c('WordOrder.Classified', 'Final.Full.WordOrder')]
#Did casemarking just produce non-embodiment?  Let's look at how closely associated the 2 are
peopledata <- alldata[alldata$Object.Type == 'Person',]
table(peopledata$PV.Embod, peopledata$SpatialCue)
#########
# GRAPHS
#########
#Copying code from above to make sure we have the right data...
ParticipantScores <- aggregate(alldata$ChoseLateral, by=list(alldata$Subject, alldata$Object.Type, alldata$GestureCondition), mean.na.rm)
names(ParticipantScores) <- c("Subject", "Object.Type", "GestureCondition", "ChoseLateral")
#Time for bootstrapped confidence intervals around the means of the 4 conditions!
PersonFree.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Person" & ParticipantScores$GestureCondition=="Free",]$ChoseLateral, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
PersonHand.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Person" & ParticipantScores$GestureCondition=="Case",]$ChoseLateral, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
ObjectFree.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Object" & ParticipantScores$GestureCondition=="Free",]$ChoseLateral, 1000, mean)
quantile(ObjectFree.boot.mean$thetastar, c(0.025, 0.975))
ObjectHand.boot.mean = bootstrap(ParticipantScores[ParticipantScores$Object.Type=="Object" & ParticipantScores$GestureCondition=="Case",]$ChoseLateral, 1000, mean)
quantile(ObjectHand.boot.mean$thetastar, c(0.025, 0.975))
GraphScores <- aggregate(ParticipantScores$ChoseLateral, by=list(ParticipantScores$Object.Type, ParticipantScores$GestureCondition), mean.na.rm)
names(GraphScores) <- c("Object.Type", "GestureCondition", "ChoseLateral")
GraphScores$errorLow = 0
GraphScores$errorHigh = 0
GraphScores[GraphScores$Object.Type == "Person" & GraphScores$GestureCondition == "Free",]$errorLow = quantile(PersonFree.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$Object.Type == "Person" & GraphScores$GestureCondition == "Free",]$errorHigh = quantile(PersonFree.boot.mean$thetastar, 0.975)
GraphScores[GraphScores$Object.Type == "Person" & GraphScores$GestureCondition == "Case",]$errorLow = quantile(PersonHand.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$Object.Type == "Person" & GraphScores$GestureCondition == "Case",]$errorHigh = quantile(PersonHand.boot.mean$thetastar, 0.975)
GraphScores[GraphScores$Object.Type == "Object" & GraphScores$GestureCondition == "Free",]$errorLow = quantile(ObjectFree.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$Object.Type == "Object" & GraphScores$GestureCondition == "Free",]$errorHigh = quantile(ObjectFree.boot.mean$thetastar, 0.975)
GraphScores[GraphScores$Object.Type == "Object" & GraphScores$GestureCondition == "Case",]$errorLow = quantile(ObjectHand.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$Object.Type == "Object" & GraphScores$GestureCondition == "Case",]$errorHigh = quantile(ObjectHand.boot.mean$thetastar, 0.975)
GraphScores$ObLabel <- ""
GraphScores[GraphScores$Object.Type == "Object",]$ObLabel <- "Inanimate patient"
GraphScores[GraphScores$Object.Type == "Person",]$ObLabel <- "Animate patient"
GraphScores$ExpLabel <- ""
GraphScores[GraphScores$GestureCondition == "Free",]$ExpLabel <- "Task 1 (Free gesture)"
GraphScores[GraphScores$GestureCondition == "Case",]$ExpLabel <- "Task 2 (Spatial instructions)"
my.cols <- brewer.pal(9, "Purples")
my.cols <- c(my.cols[6], my.cols[3])
#Fix for recalcitrant column ordering
GraphScores$ObLabel <- factor(GraphScores$ObLabel, levels = c("Inanimate patient", "Animate patient"))
GraphScores$ExpLabel <- factor(GraphScores$ExpLabel, levels = c("Task 1 (Free gesture)", "Task 2 (Spatial instructions)"))
ggplot(data=GraphScores, aes(x=ExpLabel, y=ChoseLateral, fill=ObLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
geom_errorbar(aes(ymin=errorLow, ymax=errorHigh), colour="black", width=.1, position=position_dodge(.9)) +
scale_fill_manual(values=my.cols) +
coord_cartesian(ylim=c(0,1)) +
scale_y_continuous(breaks = seq(0, 1, 0.1))+
xlab('') +
ylab('proportion of SOV-type gesture orders') +
theme_bw()
ggsave('glyphgrid_condition_vs_order.jpg')
+
theme(legend.title=element_blank())
ggplot(data=GraphScores, aes(x=ExpLabel, y=ChoseLateral, fill=ObLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
geom_errorbar(aes(ymin=errorLow, ymax=errorHigh), colour="black", width=.1, position=position_dodge(.9)) +
scale_fill_manual(values=my.cols) +
coord_cartesian(ylim=c(0,1)) +
scale_y_continuous(breaks = seq(0, 1, 0.1))+
xlab('') +
ylab('proportion of SOV-type gesture orders') +
theme_bw() +
theme(legend.title=element_blank())
ggsave('glyphgrid_condition_vs_order.jpg')
ggsave('glyphgrid_condition_vs_order.jpg')
ggplot(data=GraphScores, aes(x=ExpLabel, y=ChoseLateral, fill=ObLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
geom_errorbar(aes(ymin=errorLow, ymax=errorHigh), colour="black", width=.1, position=position_dodge(.9)) +
scale_fill_manual(values=my.cols) +
coord_cartesian(ylim=c(0,1)) +
scale_y_continuous(breaks = seq(0, 1, 0.1))+
xlab('') +
ylab('proportion of SOV-type gesture orders') +
theme_bw() +
theme(legend.title=element_blank())
ggsave('glyphgesture_condition_vs_order.jpg')
#Fix for recalcitrant column ordering
GraphScores$ObLabel <- factor(GraphScores$ObLabel, levels = c("Inanimate patient", "Animate patient"))
GraphScores$ExpLabel <- factor(GraphScores$ExpLabel, levels = c("Task 1 (Free gesture)", "Task 2 (Spatial instructions)"))
ggplot(data=GraphScores, aes(x=ExpLabel, y=ChoseLateral, fill=ObLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
geom_errorbar(aes(ymin=errorLow, ymax=errorHigh), colour="black", width=.1, position=position_dodge(.9)) +
scale_fill_manual(values=my.cols) +
coord_cartesian(ylim=c(0,1)) +
scale_y_continuous(breaks = seq(0, 1, 0.1))+
xlab('') +
ylab('proportion of SOV-type gesture orders') +
theme_bw() +
theme(legend.title=element_blank())
ggsave('gesture_condition_vs_order.jpg')
##
#responses with case vs with role conflict orders...
PeopleScores <- table(peopledata$PV.Embod, peopledata$SpatialCue)
GraphScores <- as.data.frame(PeopleScores)
names(GraphScores) <- c("PV.Embod", "SpatialCue", "count")
GraphScores$EmbodLabel <- ""
GraphScores[GraphScores$PV.Embod == TRUE,]$EmbodLabel <- "Gesture order with RCP"
GraphScores[GraphScores$PV.Embod == FALSE,]$EmbodLabel <- "Gesture order without RCP"
GraphScores$SpatLabel <- ""
GraphScores[GraphScores$SpatialCue == "Spatial.Absent",]$SpatLabel <- "Gesture with no spatial encoding"
GraphScores[GraphScores$SpatialCue == "Spatial.Present",]$SpatLabel <- "Gesture with spatial encoding"
my.cols <- brewer.pal(9, "Greens")
my.cols <- c(my.cols[6], my.cols[3])
#Fix for recalcitrant column ordering
GraphScores$EmbodLabel <- factor(GraphScores$EmbodLabel, levels = c("Gesture order with RCP", "Gesture order without RCP"))
GraphScores$SpatLabel <- factor(GraphScores$SpatLabel, levels = c("Gesture with spatial encoding","Gesture with no spatial encoding"))
ggplot(data=GraphScores, aes(x=SpatLabel, y=count, fill=EmbodLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
scale_fill_manual(values=my.cols) +
xlab('') +
ylab('Number of gesture sequences produced') +
theme_bw() +
theme(legend.title=element_blank())
ggsave('gesture_space_vs_order.jpg')
setwd("~/Dropbox/_Projects/Gesture/Gesture-GlyphGrid Repository")
#This produces all analyses for the glyphgrid study. It starts with the alldata.csv
#which is cleaned/extracted from the raw JSON produced by the experiment;
#see GlyphGridCleaning.R for those details.
rm(list = ls())
directory = getwd()
library(languageR)
library(stringr)
library(lme4)
library(multcomp)
library(binom)
library(dplyr)
library(tidyr)
library(ggplot2)
mean.na.rm <- function(x) { mean(x,na.rm=T) }
stderr <- function(x) sqrt(var(x)/length(x))
#Read in the cleaned data file (alldata), extracted from JSON output of the psiturk script.
#At this point, we have the *sequence* of cards people moved, S, V, O or X for other.
#We need to condense them for analysis, but we'll keep raw orders around for comparison.
# total of 59 addtl ppl records are excluded from analysis because their data
#couldn't be parsed from the json string (indicating missed clicks/screen refresh type issues)
#We start with 289 ppl here.
mydata <- read.csv('alldata.csv', header=T)
mydata = mydata %>%
filter(browser != "EXCLUDED") %>% #Tried to do the exp on a tablet, 3 ppl
filter(isTestTrial == 1) %>%
arrange(participant)
# Re-add stimulus info, since it didn't print from the exp
mydata$stimnum <- as.numeric(mydata$stimulus)
animate_stim <- c('girl-elbowing-oldlady.mp4','oldlady-rubbing-fireman.mp4','fireman-pushing-boy.mp4','girl-kissing-boy.mp4','girl-throwing-oldlady.mp4','fireman-kicking-girl.mp4','boy-lifting-girl.mp4')
inanimate_stim <- c('girl-pushing-car.mp4','boy-kicking-ball.mp4','fireman-lifting-car.mp4','oldlady-kissing-ball.mp4','fireman-throwing-ball.mp4','girl-pushing-ball.mp4','oldlady-elbowing-heart.mp4','girl-rubbing-heart.mp4')
intransitive_stim <-c('ball-rolling-none.mp4','girl-tumbling-none.mp4','car-tumbling-none.mp4','boy-rolling-none.mp4','car-rolling-none.mp4')
labelcat <- function(astring){
cata = "NOT FOUND"
if (astring %in% animate_stim){
cata = "Animate"
} else if (astring %in% inanimate_stim){
cata = "Inanimate"
}else if (astring %in% intransitive_stim){
cata = "Intransitive"
}
return(cata)
}
mydata$StimCategory <- mapply(labelcat, mydata$stimulus)
###
# Word order simplification
###
#Now programmatically turn the raw sequence of card moves to a three-letter description of what
#they did.  We are interested in the 1st 3 symbols moved, either S, V, or O. When participants
#moved 2 categories plus a mistake, we will analyzed a 'relaxed' version as well, assuming they
#meant that X to be the missing element.
##Get the first instance of each symbol people gave during each trial, this is the order we'll use
#NOTE: see isPerfect calc below if this bothers you.
mydata$RawOrder <- as.character(mydata$RawOrder)
long_split = strsplit(mydata$RawOrder,character(0))
short_order = list()
for (i in 1:length(long_split)){
if(mydata$RawOrder[i]!="NONE"){
short_order[[i]] = paste(unique(long_split[[i]]), collapse='')
} else {short_order[[i]] = "NONE"}
}
mydata$UniqueRawOrder = as.character(as.factor(unlist(short_order)))
##Now save up to 3 (transitive) or 2 (intransitive) glyphs, preferring SVO, allowing X if
#all reasonable glyphs weren't passed.
simpleOrderRelaxed <- function(acat, astring){
simpleStr = ""
astring = as.character(astring)
if (acat %in% c("Inanimate","Animate")){
maxlen = 3
} else if (acat %in% c("Intransitive")){
maxlen = 2
}
if (nchar(astring) > maxlen){ #we have an X (plus maybe more?) & don't need it!
simpleStr = gsub("X","",astring)
simpleStr = gsub("Y","",astring)
simpleStr = gsub("Z","",astring)
simpleStr = substr(simpleStr,1,maxlen)
} else { #We have either an X we want to keep, or no Xs
simpleStr = astring
}
return(simpleStr)
}
mydata$simpleOrderRelaxed <- mapply(simpleOrderRelaxed, mydata$StimCategory, mydata$UniqueRawOrder)
#Now make 2 versions that we'll analyze: simpleOrderStrict, which include only correct glyphs, and
#simpleOrderGenerous which replaces a single X with the missing glyph
toStrict <- function(astring){
astring <- gsub("X", "", astring)
astring <- gsub("Y", "", astring)
astring <- gsub("Z", "", astring)
}
toGenerous <- function(acat, astring){
if (acat %in% c("Inanimate","Animate")){
needed <- c("S","V","O")
} else if (acat %in% c("Intransitive")){
needed <- c("S","V")
}
#remove items until we have just ones that the string doesn't have
isMissing <- c()
for (n in needed){
if (!(str_detect(astring, n))){
isMissing <- append(isMissing, n)
}
}
#If there is exactly 1 missing, replace the X with the missing thing (if it's there)
if(length(isMissing) == 1){
astring <- gsub("X", isMissing[1],astring)
} else if(length(isMissing) > 1){ #couldn't do this one!
astring = "UNFIXABLE"
}
#(otherwise leave it as-is)
return(astring)
}
mydata$simpleOrderStrict <- mapply(toStrict, mydata$simpleOrderRelaxed)
mydata$simpleOrderGenerous <- mapply(toGenerous, mydata$StimCategory, mydata$simpleOrderRelaxed)
#Okay, so at this point we have 3 possible groups we could analyze: people whose strict simple
#order is identical to their raw order, people whose first-mention order is a valid word
#order, and people whose 'generous'/corrected first-mention order is a valid word
passesMuster <- function(acat, astring){ #"please be the correct length! Above filters ensure that there is only 1 copy of each glyph listed in sequences
if (acat %in% c("Inanimate","Animate")){
corrlen = 3
} else if (acat %in% c("Intransitive")){
corrlen = 2
}
#Make sure we don't get trimmed "unfixable" entries
if(astring == "UNFIXABLE"){
astring = ""
}
return(corrlen == nchar(astring))
}
mydata$includeStrict <- mapply(passesMuster, mydata$StimCategory, mydata$simpleOrderStrict)
mydata$includeGenerous <- mapply(passesMuster, mydata$StimCategory, mydata$simpleOrderGenerous)
mydata$includePerfect <- mydata$includeStrict & (mydata$simpleOrderStrict == mydata$RawOrder)
#The isPerfect column allowed us to check what was up with people who had a usable Strict
#condensation (i.e. we got 3/2 'real' glyphs taking the first instance of each) but who did
#not match their ORIGINAL long order.
#There are 64 such instances out of the 4,000ish total responses; some are esoteric word
#orders like SVOV, but many are things like SSOV which indicate they 'dropped' a symbol on the way
###
# Analysis plan!  We have 2 sets of responses: 'readable' responses allowing for people
# to make 1 screwup (allows us to include the most data, which we'll see is a problem),
# and then a stricter one.  We focus on the 'readable' one since it required throwing away
# less data
###
#find & filter out participants who should be excluded because they reported cheating
mydata <- mydata %>%
filter(cheated == 0) %>%
filter(simpleOrderGenerous != 'UNFIXABLE') %>%
filter(StimCategory != "Intransitive") #take just transitives!
#save the main & 'strict' rows, and narrow down to the columns relevant for each of those analyses
vars <- c('participant','trial.number', 'stimnum', 'StimCategory', 'simpleOrderStrict', 'RawOrder')
strictdata <- mydata %>%
filter(includeStrict) %>%
dplyr::select(one_of(vars))
vars <- c('participant','trial.number', 'stimnum', 'StimCategory', 'simpleOrderGenerous','RawOrder')
mydata <- mydata %>%
filter(includeGenerous) %>%
dplyr::select(one_of(vars))
#################################################################
## REPORT DESCRIPTIVES
sum.na.rm <- function(x) { sum(x,na.rm=T) }
my.sd <- function(x) {sd(x)/sqrt(length(x))}
#Report S counts (out of original 292)
subj.count = length(unique(mydata$participant)) #230
#Report counts of Word Order instances
word.order.counts = table(mydata$simpleOrderGenerous, mydata$StimCategory)
#And categorize them!
mydata$Order.Type <- ""
mydata[mydata$simpleOrderGenerous == "SOV",]$Order.Type <- "VerbLateral"
mydata[mydata$simpleOrderGenerous == "OSV",]$Order.Type <- "VerbLateral"
mydata[mydata$simpleOrderGenerous == "VSO",]$Order.Type <- "VerbLateral"
mydata[mydata$simpleOrderGenerous == "VOS",]$Order.Type <- "VerbLateral"
mydata[mydata$simpleOrderGenerous == "SVO",]$Order.Type <- "VerbMedial"
mydata[mydata$simpleOrderGenerous == "OVS",]$Order.Type <- "VerbMedial"
#Report counts of Word Order Type
order.type.counts = table(mydata$Order.Type, mydata$StimCategory)
#Make scores for each participant (we'll use these for graphing confidence intervals...)
mydata$ChoseVLat <- 0
mydata[mydata$Order.Type!="VerbMedial",]$ChoseVLat <- 1
ParticipantScores <- aggregate(mydata$ChoseVLat, by=list(mydata$participant, mydata$StimCategory), mean.na.rm)
names(ParticipantScores) <- c("participant", "ObjectType", "ChoseVLat")
ParticipantScores$ObjectType <- factor(ParticipantScores$ObjectType)
#Table for mean VLat scores, just taking a peak.
with(ParticipantScores, tapply(ChoseVLat, list(ObjectType), mean, na.rm=TRUE), drop=TRUE)
#Repeating the word order counts, let's look at people who produced a non-SVO order at some
#point in the proceedings.
mydata$participant <- as.factor(mydata$participant)
nonSVO <- aggregate(ParticipantScores$ChoseVLat, by=list(ParticipantScores$participant), sum)
names(nonSVO)<- c("participant","nonSVO")
mydata <- merge(mydata, nonSVO)
mixers <- filter(mydata, nonSVO>0)
mixer.order.counts = table(mixers$simpleOrderGenerous, mixers$StimCategory)
word.order.counts = table(mydata$simpleOrderGenerous, mydata$StimCategory)
word.order.counts #from above, all participants
mixer.order.counts
length(unique(mydata$participant))
length(unique(mixers$participant))
mixerParticipantScores <- aggregate(mixers$ChoseVLat, by=list(mixers$participant, mixers$StimCategory), mean.na.rm)
names(mixerParticipantScores) <- c("participant", "ObjectType", "ChoseVLat")
mixerParticipantScores$ObjectType <- factor(mixerParticipantScores$ObjectType)
#Simple histogram of orders
table(mydata$simpleOrderGenerous)
#########
# GRAPHS
#########
ParticipantScores <- mixerParticipantScores
#Bootstrapped confidence intervals
library(bootstrap)
animate.boot.mean = bootstrap(ParticipantScores[ParticipantScores$ObjectType=="Animate",]$ChoseVLat, 1000, mean)
quantile(animate.boot.mean$thetastar, c(0.025, 0.975))
inanimate.boot.mean = bootstrap(ParticipantScores[ParticipantScores$ObjectType=="Inanimate",]$ChoseVLat, 1000, mean)
quantile(inanimate.boot.mean$thetastar, c(0.025, 0.975))
GraphScores <- aggregate(ParticipantScores$ChoseVLat, by=list(ParticipantScores$ObjectType), mean.na.rm)
names(GraphScores) <- c("ObjectType", "ChoseVLat")
GraphScores$errorLow = 0
GraphScores$errorHigh = 0
GraphScores[GraphScores$ObjectType == "Animate",]$errorLow = quantile(animate.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$ObjectType == "Inanimate",]$errorLow = quantile(inanimate.boot.mean$thetastar, 0.025)
GraphScores[GraphScores$ObjectType == "Animate",]$errorHigh = quantile(animate.boot.mean$thetastar, 0.975)
GraphScores[GraphScores$ObjectType == "Inanimate",]$errorHigh = quantile(inanimate.boot.mean$thetastar, 0.975)
GraphScores$ObLabel <- ""
GraphScores[GraphScores$ObjectType == "Inanimate",]$ObLabel <- "Inanimate patient"
GraphScores[GraphScores$ObjectType == "Animate",]$ObLabel <- "Animate patient"
GraphScores <- filter(GraphScores, ObjectType != "Intransitive")
library(RColorBrewer)
my.cols <- brewer.pal(9, "Purples")
my.cols <- c(my.cols[6], my.cols[3])
#Fix for recalcitrant column ordering
GraphScores$ObLabel <- factor(GraphScores$ObLabel, levels = c("Inanimate patient", "Animate patient"))
ggplot(data=GraphScores, aes(x=ObLabel, y=ChoseVLat, fill=ObLabel)) +
geom_bar(position=position_dodge(), stat="identity") +
geom_errorbar(aes(ymin=errorLow, ymax=errorHigh), colour="black", width=.1, position=position_dodge(.9)) +
scale_fill_manual(values=my.cols) +
coord_cartesian(ylim=c(0,1)) +
scale_y_continuous(breaks = seq(0, 1, 0.1))+
xlab('') +
ylab('proportion of SOV-type symbol orders') +
theme_bw() +
theme(legend.title=element_blank())
ggsave('glyphgrid.jpg')
ggsave('glyphgrid.jpg')
